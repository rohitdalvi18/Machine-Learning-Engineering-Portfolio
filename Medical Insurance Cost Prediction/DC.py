# -*- coding: utf-8 -*-
"""DC1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DWbDaSYr7eEWV-vvv9gzXqjRFOCKxnSG

## Import Required Libraries:
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge, Lasso
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV

"""## Loading Our Data"""

# mount Google Drive
from google.colab import drive
import os

#Mount Google Drive
drive.mount('/content/drive', force_remount=True)

#Specify folder to access
project_folder = '/content/drive/My Drive/Colab Notebooks/Data Challenge 1: Medical Insurance Cost Prediction/'

train_data_file_path = os.path.join(project_folder, "train_data.csv")

train_data = pd.read_csv(train_data_file_path)

"""## Exploratory Data Analysis (EDA)"""

train_data.info()

train_labels_file_path = os.path.join(project_folder, "train_labels.csv")

train_labels = pd.read_csv(train_labels_file_path)

train_labels.info()

"""The dataset contains 1,070 entries with no missing values. Here’s the breakdown:

**Numerical features:** age, bmi, children, charges (target variable).

**Categorical features:** sex, smoker, region.
"""

train_data.head()

train_labels.head()

# Summary statistics for numerical features
train_data.describe()

train_labels.describe()

"""## Data Exploration"""

# Visualizing distributions
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

sns.histplot(train_data['age'], bins=20, kde=True, ax=axes[0, 0]).set(title='Age Distribution')
sns.histplot(train_data['bmi'], bins=20, kde=True, ax=axes[0, 1]).set(title='BMI Distribution')
sns.histplot(train_data['children'], bins=6, discrete=True, ax=axes[1, 0]).set(title='Children Count Distribution')
sns.histplot(train_labels['charges'], bins=20, kde=True, ax=axes[1, 1]).set(title='Charges Distribution')

plt.tight_layout()
plt.show()

"""**Observations:**


*   **Age:** Normally distributed, ranging from 18 to 64 years.

*   **BMI:** Ranges from ~16 to ~53, with a mean of 30.56, which is considered overweight.

* **Children:** Most people have 0-2 children, with a max of 5.

* **Charges:** Right-skewed distribution, indicating a few very high insurance costs.


"""

# Compute correlation matrix
correlation_matrix = train_data.copy()
correlation_matrix["charges"] = train_labels["charges"]

correlation_matrix_encoded = correlation_matrix.copy()
correlation_matrix_encoded["sex"] = correlation_matrix_encoded["sex"].map({"male": 0, "female": 1})
correlation_matrix_encoded["smoker"] = correlation_matrix_encoded["smoker"].map({"no": 0, "yes": 1})
correlation_matrix_encoded = pd.get_dummies(correlation_matrix_encoded, columns=["region"], drop_first=True)

correlation = correlation_matrix_encoded.corr()

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

# Visualizing categorical feature impact on charges
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

sns.boxplot(x=train_data["sex"], y=train_labels["charges"], ax=axes[0]).set(title="Charges by Sex")
sns.boxplot(x=train_data["smoker"], y=train_labels["charges"], ax=axes[1]).set(title="Charges by Smoking Status")
sns.boxplot(x=train_data["region"], y=train_labels["charges"], ax=axes[2]).set(title="Charges by Region")

plt.tight_layout()
plt.show()

"""**Smoking status has the strongest impact on charges –** Smokers have significantly higher medical costs.

**Sex has little impact –** The distribution of charges for males and females is similar.

**Region has minimal influence –** Charges are fairly evenly distributed across regions.

## Data Preprocessing & Encoding

### Encode categorical variables
"""

train_data_encoded = train_data.copy()
train_data_encoded["sex"] = train_data_encoded["sex"].map({"male": 0, "female": 1})
train_data_encoded["smoker"] = train_data_encoded["smoker"].map({"no": 0, "yes": 1})
train_data_encoded = pd.get_dummies(train_data_encoded, columns=["region"], drop_first=True)  # One-hot encoding for region

"""### Feature Scaling (Standardization)
Standardization or normalization for better model performance
"""

scaler = StandardScaler()
scaled_features = scaler.fit_transform(train_data_encoded)

# Convert back to DataFrame
train_data_scaled = pd.DataFrame(scaled_features, columns=train_data_encoded.columns)

# Split into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(train_data_scaled, train_labels, test_size=0.2, random_state=42)

# Display first few rows after preprocessing
train_data_scaled.head()

"""## Feature Engineering"""

# Create interaction terms
train_data_scaled["bmi_smoker"] = train_data_scaled["bmi"] * train_data_scaled["smoker"]
train_data_scaled["age_smoker"] = train_data_scaled["age"] * train_data_scaled["smoker"]

# Generate polynomial features (degree 2 for now)
poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)
poly_features = poly.fit_transform(train_data_scaled)

# Convert back to DataFrame with feature names
poly_feature_names = poly.get_feature_names_out(train_data_scaled.columns)
train_data_poly = pd.DataFrame(poly_features, columns=poly_feature_names)

# Split into train and validation sets again after feature engineering
X_train, X_val, y_train, y_val = train_test_split(train_data_poly, train_labels, test_size=0.2, random_state=42)

# Display first few rows after feature engineering
train_data_poly.head()

"""#### Interaction Features

* bmi * smoker → **bold text** Smoking might amplify the effects of BMI on charges.

* age * smoker → **bold text** Older smokers may have higher costs.

#### Polynomial Features

* Quadratic or cubic transformations of bmi or age could capture non-linearity.

#### Log Transformation

* Since charges is right-skewed, applying log(charges) might help with normality.

## Model Selection & Training

### Multiple Linear Regression
"""

# Train Multiple Linear Regression model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# Predict on validation set
y_pred = lin_reg.predict(X_val)

# Compute RMSE
RMSE = np.sqrt(mean_squared_error(y_val, y_pred))
print("Linear Regression RMSE:", RMSE)

"""###Ridge Regression with Cross-Validation"""

# Define a range of alpha values (λ) to test in Ridge Regression
alphas = np.logspace(-3, 3, 50)

# Perform Grid Search to find the best alpha (λ) for Ridge Regression
ridge_cv = GridSearchCV(Ridge(), param_grid={'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')

# Fit Ridge model on the training data using cross-validation to find best alpha
ridge_cv.fit(X_train, y_train)

print("Ridge Regression")
ridge_best_alpha = ridge_cv.best_params_['alpha']
print(f"Best Ridge Alpha: {ridge_best_alpha}")

ridge_reg = Ridge(alpha=ridge_best_alpha)
ridge_reg.fit(X_train, y_train)
y_ridge_pred = ridge_reg.predict(X_val)

ridge_rmse = np.sqrt(mean_squared_error(y_val, y_ridge_pred))
print(f"Ridge Regression RMSE: {ridge_rmse}")

"""### LASSO Regression with Cross-Validation

"""

# Perform Grid Search to find the best alpha (λ) for Lasso Regression
lasso_cv = GridSearchCV(Lasso(max_iter=10000, tol=1e-1), param_grid={'alpha': alphas}, cv=5, scoring='neg_mean_squared_error')

# Fit Lasso model on the training data using cross-validation to find best alpha
lasso_cv.fit(X_train, y_train)

# Print the best alpha (λ) value found by cross-validation
print("Lasso Regression")
lasso_best_alpha = lasso_cv.best_params_['alpha']
print(f"Best Lasso Alpha: {lasso_best_alpha}")

lasso_reg = Lasso(alpha=lasso_best_alpha)
lasso_reg.fit(X_train, y_train)
y_lasso_pred = lasso_reg.predict(X_val)
lasso_rmse = np.sqrt(mean_squared_error(y_val, y_lasso_pred))
print(f"LASSO Regression RMSE: {lasso_rmse}")

"""### Polynomial Regression with Cross-Validation"""

# Define polynomial degrees to test
degrees = range(1, 4)  # Test degrees 1 to 3
poly_reg_rmse_scores = []

# Iterate through polynomial degrees
for degree in degrees:
    # Create a pipeline with polynomial transformation and linear regression
    model = make_pipeline(PolynomialFeatures(degree, include_bias=False), LinearRegression())

    # Perform cross-validation on training data
    scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)

    # Compute RMSE
    rmse = np.mean(np.sqrt(-scores))

    # Store results
    poly_reg_rmse_scores.append(rmse)

# Plot validation curve
plt.figure(figsize=(8,5))
plt.plot(degrees, poly_reg_rmse_scores, marker='o', label="Cross-validated RMSE")
plt.xlabel("Polynomial Degree")
plt.ylabel("Score")
plt.title("Degree Selection Using Cross-Validation")
plt.legend()
plt.show()

# Select the best polynomial degree (smallest RMSE)
best_degree = degrees[np.argmin(poly_reg_rmse_scores)]
print(f"Best Polynomial Degree: {best_degree}")

poly = PolynomialFeatures(degree=best_degree, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_val_poly = poly.transform(X_val)

lin_reg_poly = LinearRegression()
lin_reg_poly.fit(X_train_poly, y_train)
y_poly_pred = lin_reg_poly.predict(X_val_poly)
poly_rmse = np.sqrt(mean_squared_error(y_val, y_poly_pred))
print(f"Polynomial Regression RMSE: {poly_rmse}")

"""Here’s what we can infer from these results:

* **Ridge Regression (RMSE: 5452.98) →** Slight improvement over plain Linear Regression (5465.61).

* **LASSO Regression (RMSE: 5429.03) →** The best so far! It suggests that some features were unnecessary and LASSO's feature selection helped.

* **Polynomial Regression (RMSE: 5465.61) →** No improvement, likely due to overfitting or unnecessary complexity.

## Final Prediction
"""

# Load test data
test_data_file_path = os.path.join(project_folder, "test_data.csv")
test_data = pd.read_csv(test_data_file_path)

# Encode categorical variable
test_data_encoded = test_data.copy()
test_data_encoded["sex"] = test_data_encoded["sex"].map({"male": 0, "female": 1})
test_data_encoded["smoker"] = test_data_encoded["smoker"].map({"no": 0, "yes": 1})
test_data_encoded = pd.get_dummies(test_data_encoded, columns=["region"], drop_first=True)  # One-hot encoding for region

# Standardization
scaler = StandardScaler()
scaled_features = scaler.fit_transform(test_data_encoded)

# Convert back to DataFrame
test_data_scaled = pd.DataFrame(scaled_features, columns=test_data_encoded.columns)

test_data_scaled.head()

# Create interaction terms
test_data_scaled["bmi_smoker"] = test_data_scaled["bmi"] * test_data_scaled["smoker"]
test_data_scaled["age_smoker"] = test_data_scaled["age"] * test_data_scaled["smoker"]

# Generate polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)
poly_features = poly.fit_transform(test_data_scaled)

# Convert back to DataFrame with feature names
poly_feature_names = poly.get_feature_names_out(test_data_scaled.columns)
test_data_poly = pd.DataFrame(poly_features, columns=poly_feature_names)

# Predict with best LASSO model
lasso_best = lasso_cv
final_predictions = lasso_best.predict(test_data_poly)

# Save final predictions
predictions_file_path = '/content/drive/My Drive/Colab Notebooks/Data Challenge 1: Medical Insurance Cost Prediction/insurance_predictions.csv'
pd.DataFrame(final_predictions).to_csv(predictions_file_path, index=False, header=False)
print(f"Predictions saved to {predictions_file_path}")

